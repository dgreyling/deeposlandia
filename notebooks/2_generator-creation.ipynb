{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at showing the data generator creation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As denoted in [other notebooks](./1a_mapillary-dataset-presentation.ipynb), one can easily generate datasets. This is a way to preprocess data offline, apart from any machine learning algorithm, in order to guarantee data persistence on the file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However running neural network analysis leads to exploit these data on-the-fly, so as to save up memory. The current notebook will bridge the gap between the `Dataset` structure and directly usable data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of clarity, we will take here a `Mapillary` dataset as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to begin the generator creation, the usual imports are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeposlandia import dataset, generator, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some global variables are created as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"../data\"\n",
    "DATASET = \"mapillary\" # \"shapes\", \"aerial\"\n",
    "MODEL = \"semantic_segmentation\" # \"feature_detection\"\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we prepare the needed folders, so as to focus thereafter on the data creation commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = utils.prepare_input_folder(DATAPATH, DATASET)\n",
    "input_data = os.path.join(input_folder, \"training\")\n",
    "input_config = os.path.join(input_folder, \"config_aggregate.json\")\n",
    "preprocess_folder = utils.prepare_preprocessed_folder(DATAPATH, DATASET, IMG_SIZE, \"aggregated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first effort, we need to create a preprocessed dataset starting from the raw data. This is done with the help of the `Dataset` class (and its derivatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instanciate an empty dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset.MapillaryDataset(IMG_SIZE, input_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To populate it, we send some input images that will be preprocessed during the operation. The preprocessed images are saved onto the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.populate(preprocess_folder[\"training\"],\n",
    "           input_data,\n",
    "           nb_images=100,\n",
    "           aggregate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the dataset information must be saved as well. They are saved as a `.json` file; this file will be indispensable when building the data generator, especially because the dataset creation and the neural network creation are not done into the same modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-22 15:01:49,563 :: INFO :: dataset :: save : The dataset has been saved into ../data/mapillary/preprocessed/128_aggregated/training.json\n"
     ]
    }
   ],
   "source": [
    "d.save(preprocess_folder[\"training_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset creation, the only needed step is to recover the preprocessed dataset information, stored previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.read_config(preprocess_folder[\"training_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the generator may be created, with some dataset and path parameters. The `create_generator` function is based on [Keras](https://keras.io/preprocessing/image/) `ImageDataGenerator` module, and more specifically to the `flow_from_directory` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 1 classes.\n",
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator.create_generator(DATASET,\n",
    "                                             MODEL,\n",
    "                                             preprocess_folder[\"training\"],\n",
    "                                             IMG_SIZE,\n",
    "                                             BATCH_SIZE,\n",
    "                                             config[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, one gets a Python generator that contains tuples of images and labels. This can be verified by extracting the first item of the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 224, 224, 3), (10, 224, 224, 13))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(train_generator)\n",
    "item[0].shape, item[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the tuple contains 10 3-channeled images of size `128*128`, whilst the second element contains 10 label arrays of size `128*128*13` (`13` being the number of aggregated labels in the dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
